{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Tut 4a -  Fine tuning a custom model ( Kaggle 5-Day Generative AI Course )\n\nToday, weâ€™ll learn how to **fine-tune** a custom model using the **Gemini API**. Fine-tuning means adjusting a pre-existing AI model to make it **work better for a specific task**, like **classifying text**. In this case, we'll teach the model to identify the category of a text from newsgroup posts.","metadata":{}},{"cell_type":"code","source":"!pip uninstall -qqy jupyterlab  # Remove unused conflicting packages\n!pip install -U -q \"google-genai==1.7.0\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-06T14:08:54.035061Z","iopub.execute_input":"2025-04-06T14:08:54.035435Z","iopub.status.idle":"2025-04-06T14:08:59.805366Z","shell.execute_reply.started":"2025-04-06T14:08:54.035402Z","shell.execute_reply":"2025-04-06T14:08:59.803592Z"}},"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Skipping jupyterlab as it is not installed.\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}],"execution_count":24},{"cell_type":"markdown","source":"### Set up your API key\n\nTo run the following cell, your API key must be stored it in a Kaggle secret named GOOGLE_API_KEY.\n\nIf you don't already have an API key, you can grab one from AI Studio. You can find detailed instructions in the docs.\n\nTo make the key available through Kaggle secrets, choose Secrets from the Add-ons menu and follow the instructions to add your key or enable it for this notebook.","metadata":{}},{"cell_type":"code","source":"import os\nfrom google import genai\nfrom google.genai import types\nfrom kaggle_secrets import UserSecretsClient\n\nGOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\nos.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n\nclient = genai.Client(api_key=GOOGLE_API_KEY)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T14:08:59.807640Z","iopub.execute_input":"2025-04-06T14:08:59.808015Z","iopub.status.idle":"2025-04-06T14:09:00.155977Z","shell.execute_reply.started":"2025-04-06T14:08:59.807975Z","shell.execute_reply":"2025-04-06T14:09:00.154788Z"}},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":"To start fine-tuning your custom model, use the `models.list` API method to find a suitable model that supports fine-tuning, refer to the model tuning documentation for more details, and then use the `TunedModel.create` API method to initiate the fine-tuning process and create your custom model.\n[https://ai.google.dev/gemini-api/docs/model-tuning/tutorial?lang=python](http://)","metadata":{}},{"cell_type":"markdown","source":"## Download the dataset\n###  In this activity, you will use the fine-tuned Gemini model to train/text same newsgroups dataset. The 20 Newsgroups Text Dataset contains 18,000 newsgroups posts on 20 topics divided into training and test sets.","metadata":{}},{"cell_type":"code","source":"for model in client.models.list():\n    if \"createTunedModel\" in model.supported_actions:\n        print(model.name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T14:09:00.161181Z","iopub.execute_input":"2025-04-06T14:09:00.161503Z","iopub.status.idle":"2025-04-06T14:09:00.218337Z","shell.execute_reply.started":"2025-04-06T14:09:00.161478Z","shell.execute_reply":"2025-04-06T14:09:00.216770Z"}},"outputs":[{"name":"stdout","text":"models/gemini-1.5-flash-001-tuning\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"from sklearn.datasets import fetch_20newsgroups\n\nnewsgroups_train = fetch_20newsgroups(subset=\"train\")\nnewsgroups_test = fetch_20newsgroups(subset=\"test\")\n\n# View list of class names for dataset\nnewsgroups_train.target_names\n\nprint(newsgroups_train.data[0])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T14:09:00.220259Z","iopub.execute_input":"2025-04-06T14:09:00.220701Z","iopub.status.idle":"2025-04-06T14:09:00.922552Z","shell.execute_reply.started":"2025-04-06T14:09:00.220657Z","shell.execute_reply":"2025-04-06T14:09:00.921027Z"}},"outputs":[{"name":"stdout","text":"From: lerxst@wam.umd.edu (where's my thing)\nSubject: WHAT car is this!?\nNntp-Posting-Host: rac3.wam.umd.edu\nOrganization: University of Maryland, College Park\nLines: 15\n\n I was wondering if anyone out there could enlighten me on this car I saw\nthe other day. It was a 2-door sports car, looked to be from the late 60s/\nearly 70s. It was called a Bricklin. The doors were really small. In addition,\nthe front bumper was separate from the rest of the body. This is \nall I know. If anyone can tellme a model name, engine specs, years\nof production, where this car is made, history, or whatever info you\nhave on this funky looking car, please e-mail.\n\nThanks,\n- IL\n   ---- brought to you by your neighborhood Lerxst ----\n\n\n\n\n\n","output_type":"stream"}],"execution_count":27},{"cell_type":"markdown","source":"### Why this cleaning is important:\n\nThis code prepares text data from a newsgroup dataset (think online forums) for use in a machine learning model. It focuses on cleaning and formatting the text to make it more suitable for analysis.","metadata":{}},{"cell_type":"code","source":"import email\nimport re\n\nimport pandas as pd\n\n\ndef preprocess_newsgroup_row(data):\n    # Extract only the subject and body\n    msg = email.message_from_string(data)\n    text = f\"{msg['Subject']}\\n\\n{msg.get_payload()}\"\n    # Strip any remaining email addresses\n    text = re.sub(r\"[\\w\\.-]+@[\\w\\.-]+\", \"\", text)\n    # Truncate the text to fit within the input limits\n    text = text[:40000]\n\n    return text\n\n\ndef preprocess_newsgroup_data(newsgroup_dataset):\n    # Put data points into dataframe\n    df = pd.DataFrame(\n        {\"Text\": newsgroup_dataset.data, \"Label\": newsgroup_dataset.target}\n    )\n    # Clean up the text\n    df[\"Text\"] = df[\"Text\"].apply(preprocess_newsgroup_row)\n    # Match label to target name index\n    df[\"Class Name\"] = df[\"Label\"].map(lambda l: newsgroup_dataset.target_names[l])\n\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T14:09:00.923784Z","iopub.execute_input":"2025-04-06T14:09:00.924354Z","iopub.status.idle":"2025-04-06T14:09:00.931398Z","shell.execute_reply.started":"2025-04-06T14:09:00.924316Z","shell.execute_reply":"2025-04-06T14:09:00.930110Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"# Apply preprocessing to training and test datasets\ndf_train = preprocess_newsgroup_data(newsgroups_train)\ndf_test = preprocess_newsgroup_data(newsgroups_test)\n\ndf_train.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T14:09:00.932706Z","iopub.execute_input":"2025-04-06T14:09:00.933196Z","iopub.status.idle":"2025-04-06T14:09:04.773080Z","shell.execute_reply.started":"2025-04-06T14:09:00.933151Z","shell.execute_reply":"2025-04-06T14:09:04.772134Z"}},"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"                                                Text  Label  \\\n0  WHAT car is this!?\\n\\n I was wondering if anyo...      7   \n1  SI Clock Poll - Final Call\\n\\nA fair number of...      4   \n2  PB questions...\\n\\nwell folks, my mac plus fin...      4   \n3  Re: Weitek P9000 ?\\n\\nRobert J.C. Kyanko () wr...      1   \n4  Re: Shuttle Launch Question\\n\\nFrom article <>...     14   \n\n              Class Name  \n0              rec.autos  \n1  comp.sys.mac.hardware  \n2  comp.sys.mac.hardware  \n3          comp.graphics  \n4              sci.space  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Text</th>\n      <th>Label</th>\n      <th>Class Name</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>WHAT car is this!?\\n\\n I was wondering if anyo...</td>\n      <td>7</td>\n      <td>rec.autos</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>SI Clock Poll - Final Call\\n\\nA fair number of...</td>\n      <td>4</td>\n      <td>comp.sys.mac.hardware</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>PB questions...\\n\\nwell folks, my mac plus fin...</td>\n      <td>4</td>\n      <td>comp.sys.mac.hardware</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Re: Weitek P9000 ?\\n\\nRobert J.C. Kyanko () wr...</td>\n      <td>1</td>\n      <td>comp.graphics</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Re: Shuttle Launch Question\\n\\nFrom article &lt;&gt;...</td>\n      <td>14</td>\n      <td>sci.space</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":29},{"cell_type":"markdown","source":"**Sampling the Data for Training**\n\n* Select a small, representative subset of our data.\n* Specifically, we'll keep 50 data points (messages) from each category.\n* This small sample size is sufficient because we're using a parameter-efficient fine-tuning (PEFT) technique.\n* PEFT allows us to train effectively with fewer examples.","metadata":{}},{"cell_type":"code","source":"def sample_data(df, num_samples, classes_to_keep):\n    # Sample rows, selecting num_samples of each Label.\n    df = (\n        df.groupby(\"Label\")[df.columns]\n        .apply(lambda x: x.sample(num_samples))\n        .reset_index(drop=True)\n    )\n\n    df = df[df[\"Class Name\"].str.contains(classes_to_keep)]\n    df[\"Class Name\"] = df[\"Class Name\"].astype(\"category\")\n\n    return df\n\n\nTRAIN_NUM_SAMPLES = 50\nTEST_NUM_SAMPLES = 10\n# Keep rec.* and sci.*\nCLASSES_TO_KEEP = \"^rec|^sci\"\n\ndf_train = sample_data(df_train, TRAIN_NUM_SAMPLES, CLASSES_TO_KEEP)\ndf_test = sample_data(df_test, TEST_NUM_SAMPLES, CLASSES_TO_KEEP)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T14:09:04.774204Z","iopub.execute_input":"2025-04-06T14:09:04.774577Z","iopub.status.idle":"2025-04-06T14:09:04.823989Z","shell.execute_reply.started":"2025-04-06T14:09:04.774548Z","shell.execute_reply":"2025-04-06T14:09:04.822966Z"}},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":"## Baseline Evaluation: Sample Inspection\n\nTo gauge tuning effectiveness, we first inspect a sample from the test data.\n\n1.  **Select Sample:** We pick a row (e.g., the first).\n2.  **Preprocess & Label:** We process the text and get its category.\n3.  **Display:** We show the processed text and its label.\n\nThis helps us understand the data and verify labels before model tuning.","metadata":{}},{"cell_type":"code","source":"sample_idx = 0\nsample_row = preprocess_newsgroup_row(newsgroups_test.data[sample_idx])\nsample_label = newsgroups_test.target_names[newsgroups_test.target[sample_idx]]\n\nprint(sample_row)\nprint('---')\nprint('Label:', sample_label)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T14:09:04.826697Z","iopub.execute_input":"2025-04-06T14:09:04.827032Z","iopub.status.idle":"2025-04-06T14:09:04.833482Z","shell.execute_reply.started":"2025-04-06T14:09:04.827006Z","shell.execute_reply":"2025-04-06T14:09:04.832442Z"}},"outputs":[{"name":"stdout","text":"Need info on 88-89 Bonneville\n\n\n I am a little confused on all of the models of the 88-89 bonnevilles.\nI have heard of the LE SE LSE SSE SSEI. Could someone tell me the\ndifferences are far as features or performance. I am also curious to\nknow what the book value is for prefereably the 89 model. And how much\nless than book value can you usually get them for. In other words how\nmuch are they in demand this time of year. I have heard that the mid-spring\nearly summer is the best time to buy.\n\n\t\t\tNeil Gandler\n\n---\nLabel: rec.autos\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"response = client.models.generate_content(\n    model=\"gemini-1.5-flash-001\", contents=sample_row)\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T14:09:04.835088Z","iopub.execute_input":"2025-04-06T14:09:04.835387Z","iopub.status.idle":"2025-04-06T14:09:08.607151Z","shell.execute_reply.started":"2025-04-06T14:09:04.835359Z","shell.execute_reply":"2025-04-06T14:09:08.605910Z"}},"outputs":[{"name":"stdout","text":"##  Bonneville Models Explained (1988-1989)\n\nYou're right, the Bonneville lineup was a bit of a jungle in those years. Here's a breakdown:\n\n**Base Bonneville:**  The basic, no-frills model. It still had a powerful engine, but lacked many of the luxuries of the higher trims.\n\n**LE (Luxury Edition):**  Added comfort and convenience features like power windows, locks, and mirrors. Often came with cloth upholstery.\n\n**SE (Special Edition):**  A sporty trim with a more aggressive look. Featured a unique grille, wheels, and sometimes a slightly more powerful engine.\n\n**LSE (Luxury Sport Edition):**  Combined the luxury of the LE with the sporty features of the SE. It was the ultimate \"best of both worlds\" Bonneville.\n\n**SSE (Sport Sedan Edition):**  A true performance version. The SSE came with a more powerful engine, sporty suspension, and often had a special appearance package.\n\n**SSEi (Sport Sedan Edition with Injection):**  The top-of-the-line Bonneville. The SSEi received the most powerful engine option (usually a 3.8L V6) and included features like a limited-slip differential and anti-lock brakes. \n\n**Key Differences:**\n\n* **Engines:** The primary difference between the models often boiled down to engine size and power output. \n* **Features:** Higher trim levels added features like leather seats, power seats, sunroof, and even digital instrumentation. \n* **Appearance:** Certain trims came with unique bodywork, wheels, and interior trim to differentiate them from the rest of the pack.\n\n**Book Value and Demand:**\n\nUnfortunately, providing an accurate book value for a 1989 Bonneville is impossible without more specifics. The condition, mileage, and specific trim level all drastically influence the value.\n\n* **Resources:** You can use websites like Kelley Blue Book (KBB), Edmunds, or Hagerty to get an idea of the value.\n* **Condition:**  A well-maintained 1989 Bonneville in excellent condition will fetch more than a neglected one with high mileage.\n* **Demand:**  While mid-spring and early summer are often seen as favorable times to buy, demand for older cars like the Bonneville can fluctuate. It's best to research local market trends.\n\n**Finding a Deal:**\n\n* **Private sellers:** You're more likely to find a good deal on a used car from an individual seller. However, be cautious and thoroughly inspect the car before purchasing.\n* **Used car dealerships:** Dealerships offer a level of convenience, but prices are often higher. \n* **Online marketplaces:** Websites like Craigslist, Facebook Marketplace, or Autotrader can provide a wider selection of options.\n\n**Final Tip:**\n\nDon't be afraid to negotiate! Research comparable cars and be prepared to walk away if the price isn't right. \n\n","output_type":"stream"}],"execution_count":32},{"cell_type":"markdown","source":"a.  **Prompt Definition:**\n    * `prompt = \"From what newsgroup does the following message originate?\"`\n\nb.  **Model Interaction:**\n    * `baseline_response = client.models.generate_content(...)`\n\nc.  **Output Display:**\n    * `print(baseline_response.text)`","metadata":{}},{"cell_type":"code","source":"# Ask the model directly in a zero-shot prompt.\n\nprompt = \"From what newsgroup does the following message originate?\"\nbaseline_response = client.models.generate_content(\n    model=\"gemini-1.5-flash-001\",\n    contents=[prompt, sample_row])\nprint(baseline_response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T14:09:08.608206Z","iopub.execute_input":"2025-04-06T14:09:08.608504Z","iopub.status.idle":"2025-04-06T14:09:09.915462Z","shell.execute_reply.started":"2025-04-06T14:09:08.608476Z","shell.execute_reply":"2025-04-06T14:09:09.914319Z"}},"outputs":[{"name":"stdout","text":"While the message doesn't explicitly state the newsgroup, it's highly likely it originated from **rec.autos.pontiac**, a newsgroup dedicated to discussions about Pontiac cars. \n\nHere's why:\n\n* **Topic:** The message specifically focuses on Pontiac Bonneville models from the 1988-1989 years.\n* **Jargon:** The use of model designations like LE, SE, LSE, SSE, and SSEi are specific to Pontiac Bonneville models of that era.\n* **Common Interest:**  People interested in buying or learning about a particular Pontiac model would likely seek information in a newsgroup dedicated to the brand.\n\nTherefore, **rec.autos.pontiac** is the most likely origin of this message. \n\n","output_type":"stream"}],"execution_count":33},{"cell_type":"markdown","source":"## **Retry Mechanism:**","metadata":{}},{"cell_type":"code","source":"from google.api_core import retry\n\n# You can use a system instruction to do more direct prompting, and get a\n# more succinct answer.\n\nsystem_instruct = \"\"\"\nYou are a classification service. You will be passed input that represents\na newsgroup post and you must respond with the newsgroup from which the post\noriginates.\n\"\"\"\n\n# Define a helper to retry when per-minute quota is reached.\nis_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n\n# If you want to evaluate your own technique, replace this body of this function\n# with your model, prompt and other code and return the predicted answer.\n@retry.Retry(predicate=is_retriable)\ndef predict_label(post: str) -> str:\n    response = client.models.generate_content(\n        model=\"gemini-1.5-flash-001\",\n        config=types.GenerateContentConfig(\n            system_instruction=system_instruct),\n        contents=post)\n\n    rc = response.candidates[0]\n\n    # Any errors, filters, recitation, etc we can mark as a general error\n    if rc.finish_reason.name != \"STOP\":\n        return \"(error)\"\n    else:\n        # Clean up the response.\n        return response.text.strip()\n\n\nprediction = predict_label(sample_row)\n\nprint(prediction)\nprint()\nprint(\"Correct!\" if prediction == sample_label else \"Incorrect.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T14:09:09.916459Z","iopub.execute_input":"2025-04-06T14:09:09.916769Z","iopub.status.idle":"2025-04-06T14:09:10.556041Z","shell.execute_reply.started":"2025-04-06T14:09:09.916742Z","shell.execute_reply":"2025-04-06T14:09:10.554856Z"}},"outputs":[{"name":"stdout","text":"rec.autos.misc\n\nIncorrect.\n","output_type":"stream"}],"execution_count":34},{"cell_type":"markdown","source":"## Evaluates the performance of the `predict_label` function (from the previous example) on a subset of test data.\n","metadata":{}},{"cell_type":"code","source":"import tqdm\nfrom tqdm.rich import tqdm as tqdmr\nimport warnings\n\n# Enable tqdm features on Pandas.\ntqdmr.pandas()\n\n# But suppress the experimental warning\nwarnings.filterwarnings(\"ignore\", category=tqdm.TqdmExperimentalWarning)\n\n\n# Further sample the test data to be mindful of the free-tier quota.\ndf_baseline_eval = sample_data(df_test, 2, '.*')\n\n# Make predictions using the sampled data.\ndf_baseline_eval['Prediction'] = df_baseline_eval['Text'].progress_apply(predict_label)\n\n# And calculate the accuracy.\naccuracy = (df_baseline_eval[\"Class Name\"] == df_baseline_eval[\"Prediction\"]).sum() / len(df_baseline_eval)\nprint(f\"Accuracy: {accuracy:.2%}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T14:09:10.557104Z","iopub.execute_input":"2025-04-06T14:09:10.557417Z","iopub.status.idle":"2025-04-06T14:10:43.724803Z","shell.execute_reply.started":"2025-04-06T14:09:10.557387Z","shell.execute_reply":"2025-04-06T14:10:43.723572Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output()","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb7c0bdac17c4d98b6d6fe939423027b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"},"metadata":{}},{"name":"stdout","text":"Accuracy: 18.75%\n","output_type":"stream"}],"execution_count":35},{"cell_type":"markdown","source":"### Prepares training data and either reuses an existing tuned model or queues a new tuning job for newsgroup classification using Gemini 1.5 Flash.\n\n**Here's a breakdown:**\n\n1.  **Data Preparation:**\n    * `input_data` converts the training data (`df_train`) into a dictionary format suitable for model tuning.\n\n2.  **Model ID Handling:**\n    * `model_id` stores the ID of the tuned model to be used.\n\n3.  **Tuning Job Submission:**\n    * If no existing `model_id` was found, it submits a new tuning job:\n\n4.  **Output:**\n    * `print(model_id)` prints the `model_id`, which is either the ID of an existing tuned model or the ID of the newly queued tuning job.","metadata":{}},{"cell_type":"code","source":"from collections.abc import Iterable\nimport random\n\n\n# Convert the data frame into a dataset suitable for tuning.\ninput_data = {'examples': \n    df_train[['Text', 'Class Name']]\n      .rename(columns={'Text': 'textInput', 'Class Name': 'output'})\n      .to_dict(orient='records')\n }\n\n# If you are re-running this lab, add your model_id here.\nmodel_id = None\n\n# Or try and find a recent tuning job.\nif not model_id:\n  queued_model = None\n  # Newest models first.\n  for m in reversed(client.tunings.list()):\n    # Only look at newsgroup classification models.\n    if m.name.startswith('tunedModels/newsgroup-classification-model'):\n      # If there is a completed model, use the first (newest) one.\n      if m.state.name == 'JOB_STATE_SUCCEEDED':\n        model_id = m.name\n        print('Found existing tuned model to reuse.')\n        break\n\n      elif m.state.name == 'JOB_STATE_RUNNING' and not queued_model:\n        # If there's a model still queued, remember the most recent one.\n        queued_model = m.name\n  else:\n    if queued_model:\n      model_id = queued_model\n      print('Found queued model, still waiting.')\n\n\n# Upload the training data and queue the tuning job.\nif not model_id:\n    tuning_op = client.tunings.tune(\n        base_model=\"models/gemini-1.5-flash-001-tuning\",\n        training_dataset=input_data,\n        config=types.CreateTuningJobConfig(\n            tuned_model_display_name=\"Newsgroup classification model\",\n            batch_size=16,\n            epoch_count=2,\n        ),\n    )\n\n    print(tuning_op.state)\n    model_id = tuning_op.name\n\nprint(model_id)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T14:10:43.725937Z","iopub.execute_input":"2025-04-06T14:10:43.726233Z","iopub.status.idle":"2025-04-06T14:10:44.895351Z","shell.execute_reply.started":"2025-04-06T14:10:43.726206Z","shell.execute_reply":"2025-04-06T14:10:44.894344Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-36-109541ad15be>:39: ExperimentalWarning: The SDK's tuning implementation is experimental, and may change in future versions.\n  tuning_op = client.tunings.tune(\n","output_type":"stream"},{"name":"stdout","text":"JobState.JOB_STATE_QUEUED\ntunedModels/newsgroup-classification-model-v03myfqwv\n","output_type":"stream"}],"execution_count":36},{"cell_type":"markdown","source":"### Waits for a model tuning job to complete and handles potential timeouts or errors.\n\n1.  **Initialization:**\n    * `MAX_WAIT` defines the maximum time to wait for the tuning job (10 minutes).\n2.  **Waiting and Status Updates:**\n3.  **Timeout Handling:**\n4.  **Completion and Error Handling:**\n\nThe code waits for a model tuning job to finish, providing status updates. If the job takes too long, it switches to a pre-trained model to avoid waiting indefinitely. It also handles and reports any errors that occurred during the tuning process.","metadata":{}},{"cell_type":"code","source":"import datetime\nimport time\n\n\nMAX_WAIT = datetime.timedelta(minutes=10)\n\nwhile not (tuned_model := client.tunings.get(name=model_id)).has_ended:\n\n    print(tuned_model.state)\n    time.sleep(60)\n\n    # Don't wait too long. Use a public model if this is going to take a while.\n    if datetime.datetime.now(datetime.timezone.utc) - tuned_model.create_time > MAX_WAIT:\n        print(\"Taking a shortcut, using a previously prepared model.\")\n        model_id = \"tunedModels/newsgroup-classification-model-ltenbi1b\"\n        tuned_model = client.tunings.get(name=model_id)\n        break\n\n\nprint(f\"Done! The model state is: {tuned_model.state.name}\")\n\nif not tuned_model.has_succeeded and tuned_model.error:\n    print(\"Error:\", tuned_model.error)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T14:10:44.896459Z","iopub.execute_input":"2025-04-06T14:10:44.896844Z","iopub.status.idle":"2025-04-06T14:13:03.028276Z","shell.execute_reply.started":"2025-04-06T14:10:44.896808Z","shell.execute_reply":"2025-04-06T14:13:03.026299Z"}},"outputs":[{"name":"stdout","text":"JobState.JOB_STATE_RUNNING\nJobState.JOB_STATE_RUNNING\nJobState.JOB_STATE_RUNNING\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-37-dceb2699bfb0>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuned_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Don't wait too long. Use a public model if this is going to take a while.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":37},{"cell_type":"code","source":"new_text = \"\"\"\nFirst-timer looking to get out of here.\n\nHi, I'm writing about my interest in travelling to the outer limits!\n\nWhat kind of craft can I buy? What is easiest to access from this 3rd rock?\n\nLet me know how to do that please.\n\"\"\"\n\nresponse = client.models.generate_content(\n    model=model_id, contents=new_text)\n\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T14:13:07.244036Z","iopub.execute_input":"2025-04-06T14:13:07.244379Z","iopub.status.idle":"2025-04-06T14:13:07.458827Z","shell.execute_reply.started":"2025-04-06T14:13:07.244350Z","shell.execute_reply":"2025-04-06T14:13:07.457167Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-38-ba39224469b0>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \"\"\"\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m response = client.models.generate_content(\n\u001b[0m\u001b[1;32m     12\u001b[0m     model=model_id, contents=new_text)\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/genai/models.py\u001b[0m in \u001b[0;36mgenerate_content\u001b[0;34m(self, model, contents, config)\u001b[0m\n\u001b[1;32m   5368\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mremaining_remote_calls_afc\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5369\u001b[0m       \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5370\u001b[0;31m       response = self._generate_content(\n\u001b[0m\u001b[1;32m   5371\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5372\u001b[0m       )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/genai/models.py\u001b[0m in \u001b[0;36m_generate_content\u001b[0;34m(self, model, contents, config)\u001b[0m\n\u001b[1;32m   4336\u001b[0m     \u001b[0mrequest_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_common\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_unserializable_types\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4338\u001b[0;31m     response_dict = self._api_client.request(\n\u001b[0m\u001b[1;32m   4339\u001b[0m         \u001b[0;34m'post'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhttp_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4340\u001b[0m     )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/genai/_api_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, http_method, path, request_dict, http_options)\u001b[0m\n\u001b[1;32m    638\u001b[0m         \u001b[0mhttp_method\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhttp_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m     )\n\u001b[0;32m--> 640\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_request\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    641\u001b[0m     \u001b[0mjson_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mjson_response\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/genai/_api_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, http_request, stream)\u001b[0m\n\u001b[1;32m    567\u001b[0m           \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhttp_request\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m       )\n\u001b[0;32m--> 569\u001b[0;31m       \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAPIError\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    570\u001b[0m       return HttpResponse(\n\u001b[1;32m    571\u001b[0m           \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mstream\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/genai/errors.py\u001b[0m in \u001b[0;36mraise_for_response\u001b[0;34m(cls, response)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0mstatus_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;36m400\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mstatus_code\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mClientError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;36m500\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mstatus_code\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m600\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mServerError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mClientError\u001b[0m: 400 INVALID_ARGUMENT. {'error': {'code': 400, 'message': 'Tuned model tunedModels/newsgroup-classification-model-v03myfqwv is not ready to use.', 'status': 'INVALID_ARGUMENT'}}"],"ename":"ClientError","evalue":"400 INVALID_ARGUMENT. {'error': {'code': 400, 'message': 'Tuned model tunedModels/newsgroup-classification-model-v03myfqwv is not ready to use.', 'status': 'INVALID_ARGUMENT'}}","output_type":"error"}],"execution_count":38},{"cell_type":"markdown","source":"\n1.  **Baseline Model Input Cost:**\n2.  **Tuned Model Input Cost:**\n3.  **Token Savings Calculation:**","metadata":{}},{"cell_type":"code","source":"# Calculate the *input* cost of the baseline model with system instructions.\nsysint_tokens = client.models.count_tokens(\n    model='gemini-1.5-flash-001', contents=[system_instruct, sample_row]\n).total_tokens\nprint(f'System instructed baseline model: {sysint_tokens} (input)')\n\n# Calculate the input cost of the tuned model.\ntuned_tokens = client.models.count_tokens(model=tuned_model.base_model, contents=sample_row).total_tokens\nprint(f'Tuned model: {tuned_tokens} (input)')\n\nsavings = (sysint_tokens - tuned_tokens) / tuned_tokens\nprint(f'Token savings: {savings:.2%}')  # Note that this is only n=1.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T14:13:16.541859Z","iopub.execute_input":"2025-04-06T14:13:16.542217Z","iopub.status.idle":"2025-04-06T14:13:16.678385Z","shell.execute_reply.started":"2025-04-06T14:13:16.542187Z","shell.execute_reply":"2025-04-06T14:13:16.677142Z"}},"outputs":[{"name":"stdout","text":"System instructed baseline model: 172 (input)\nTuned model: 136 (input)\nToken savings: 26.47%\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"baseline_token_output = baseline_response.usage_metadata.candidates_token_count\nprint('Baseline (verbose) output tokens:', baseline_token_output)\n\ntuned_model_output = client.models.generate_content(\n    model=model_id, contents=sample_row)\ntuned_tokens_output = tuned_model_output.usage_metadata.candidates_token_count\nprint('Tuned output tokens:', tuned_tokens_output)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T14:13:18.952445Z","iopub.execute_input":"2025-04-06T14:13:18.952814Z","iopub.status.idle":"2025-04-06T14:13:19.127217Z","shell.execute_reply.started":"2025-04-06T14:13:18.952782Z","shell.execute_reply":"2025-04-06T14:13:19.125603Z"}},"outputs":[{"name":"stdout","text":"Baseline (verbose) output tokens: 157\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-40-88969e213d80>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Baseline (verbose) output tokens:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaseline_token_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m tuned_model_output = client.models.generate_content(\n\u001b[0m\u001b[1;32m      5\u001b[0m     model=model_id, contents=sample_row)\n\u001b[1;32m      6\u001b[0m \u001b[0mtuned_tokens_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuned_model_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musage_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcandidates_token_count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/genai/models.py\u001b[0m in \u001b[0;36mgenerate_content\u001b[0;34m(self, model, contents, config)\u001b[0m\n\u001b[1;32m   5368\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mremaining_remote_calls_afc\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5369\u001b[0m       \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5370\u001b[0;31m       response = self._generate_content(\n\u001b[0m\u001b[1;32m   5371\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5372\u001b[0m       )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/genai/models.py\u001b[0m in \u001b[0;36m_generate_content\u001b[0;34m(self, model, contents, config)\u001b[0m\n\u001b[1;32m   4336\u001b[0m     \u001b[0mrequest_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_common\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_unserializable_types\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4338\u001b[0;31m     response_dict = self._api_client.request(\n\u001b[0m\u001b[1;32m   4339\u001b[0m         \u001b[0;34m'post'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhttp_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4340\u001b[0m     )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/genai/_api_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, http_method, path, request_dict, http_options)\u001b[0m\n\u001b[1;32m    638\u001b[0m         \u001b[0mhttp_method\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhttp_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m     )\n\u001b[0;32m--> 640\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_request\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    641\u001b[0m     \u001b[0mjson_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mjson_response\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/genai/_api_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, http_request, stream)\u001b[0m\n\u001b[1;32m    567\u001b[0m           \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhttp_request\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m       )\n\u001b[0;32m--> 569\u001b[0;31m       \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAPIError\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    570\u001b[0m       return HttpResponse(\n\u001b[1;32m    571\u001b[0m           \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mstream\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/genai/errors.py\u001b[0m in \u001b[0;36mraise_for_response\u001b[0;34m(cls, response)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0mstatus_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;36m400\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mstatus_code\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mClientError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;36m500\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mstatus_code\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m600\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mServerError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mClientError\u001b[0m: 400 INVALID_ARGUMENT. {'error': {'code': 400, 'message': 'Tuned model tunedModels/newsgroup-classification-model-v03myfqwv is not ready to use.', 'status': 'INVALID_ARGUMENT'}}"],"ename":"ClientError","evalue":"400 INVALID_ARGUMENT. {'error': {'code': 400, 'message': 'Tuned model tunedModels/newsgroup-classification-model-v03myfqwv is not ready to use.', 'status': 'INVALID_ARGUMENT'}}","output_type":"error"}],"execution_count":40},{"cell_type":"code","source":"# Calculate the *input* cost of the baseline model with system instructions.\nsysint_tokens = client.models.count_tokens(\n    model='gemini-1.5-flash-001', contents=[system_instruct, sample_row]\n).total_tokens\nprint(f'System instructed baseline model: {sysint_tokens} (input)')\n\n# Calculate the input cost of the tuned model.\ntuned_tokens = client.models.count_tokens(model=tuned_model.base_model, contents=sample_row).total_tokens\nprint(f'Tuned model: {tuned_tokens} (input)')\n\nsavings = (sysint_tokens - tuned_tokens) / tuned_tokens\nprint(f'Token savings: {savings:.2%}')  # Note that this is only n=1.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T14:13:25.186094Z","iopub.execute_input":"2025-04-06T14:13:25.186458Z","iopub.status.idle":"2025-04-06T14:13:25.307417Z","shell.execute_reply.started":"2025-04-06T14:13:25.186427Z","shell.execute_reply":"2025-04-06T14:13:25.305965Z"}},"outputs":[{"name":"stdout","text":"System instructed baseline model: 172 (input)\nTuned model: 136 (input)\nToken savings: 26.47%\n","output_type":"stream"}],"execution_count":41},{"cell_type":"code","source":"baseline_token_output = baseline_response.usage_metadata.candidates_token_count\nprint('Baseline (verbose) output tokens:', baseline_token_output)\n\ntuned_model_output = client.models.generate_content(\n    model=model_id, contents=sample_row)\ntuned_tokens_output = tuned_model_output.usage_metadata.candidates_token_count\nprint('Tuned output tokens:', tuned_tokens_output)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T14:13:28.638096Z","iopub.execute_input":"2025-04-06T14:13:28.638469Z","iopub.status.idle":"2025-04-06T14:13:28.805905Z","shell.execute_reply.started":"2025-04-06T14:13:28.638438Z","shell.execute_reply":"2025-04-06T14:13:28.804164Z"}},"outputs":[{"name":"stdout","text":"Baseline (verbose) output tokens: 157\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-42-88969e213d80>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Baseline (verbose) output tokens:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaseline_token_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m tuned_model_output = client.models.generate_content(\n\u001b[0m\u001b[1;32m      5\u001b[0m     model=model_id, contents=sample_row)\n\u001b[1;32m      6\u001b[0m \u001b[0mtuned_tokens_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuned_model_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musage_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcandidates_token_count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/genai/models.py\u001b[0m in \u001b[0;36mgenerate_content\u001b[0;34m(self, model, contents, config)\u001b[0m\n\u001b[1;32m   5368\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mremaining_remote_calls_afc\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5369\u001b[0m       \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5370\u001b[0;31m       response = self._generate_content(\n\u001b[0m\u001b[1;32m   5371\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5372\u001b[0m       )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/genai/models.py\u001b[0m in \u001b[0;36m_generate_content\u001b[0;34m(self, model, contents, config)\u001b[0m\n\u001b[1;32m   4336\u001b[0m     \u001b[0mrequest_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_common\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_unserializable_types\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4338\u001b[0;31m     response_dict = self._api_client.request(\n\u001b[0m\u001b[1;32m   4339\u001b[0m         \u001b[0;34m'post'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhttp_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4340\u001b[0m     )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/genai/_api_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, http_method, path, request_dict, http_options)\u001b[0m\n\u001b[1;32m    638\u001b[0m         \u001b[0mhttp_method\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhttp_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m     )\n\u001b[0;32m--> 640\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_request\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    641\u001b[0m     \u001b[0mjson_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mjson_response\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/genai/_api_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, http_request, stream)\u001b[0m\n\u001b[1;32m    567\u001b[0m           \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhttp_request\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m       )\n\u001b[0;32m--> 569\u001b[0;31m       \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAPIError\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    570\u001b[0m       return HttpResponse(\n\u001b[1;32m    571\u001b[0m           \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mstream\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/genai/errors.py\u001b[0m in \u001b[0;36mraise_for_response\u001b[0;34m(cls, response)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0mstatus_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;36m400\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mstatus_code\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mClientError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;36m500\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mstatus_code\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m600\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mServerError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mClientError\u001b[0m: 400 INVALID_ARGUMENT. {'error': {'code': 400, 'message': 'Tuned model tunedModels/newsgroup-classification-model-v03myfqwv is not ready to use.', 'status': 'INVALID_ARGUMENT'}}"],"ename":"ClientError","evalue":"400 INVALID_ARGUMENT. {'error': {'code': 400, 'message': 'Tuned model tunedModels/newsgroup-classification-model-v03myfqwv is not ready to use.', 'status': 'INVALID_ARGUMENT'}}","output_type":"error"}],"execution_count":42},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}