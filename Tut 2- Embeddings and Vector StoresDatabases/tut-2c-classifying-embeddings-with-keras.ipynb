{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Tut 2c- Classifying embeddings with Keras (Kaggle 5-day Generative AI course.)\n\n### Overview\nIn this notebook, you'll learn to use the embeddings produced by the Gemini API to train a model that can classify newsgroup posts into the categories (the newsgroup itself) from the post contents.\n\nThis technique uses the Gemini API's embeddings as input, avoiding the need to train on text input directly, and as a result it is able to perform quite well using relatively few examples compared to training a text model from scratch.","metadata":{}},{"cell_type":"code","source":"!pip install -U -q \"google-genai==1.7.0\"\nfrom google import genai\nfrom google.genai import types\n\ngenai.__version__","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T07:40:47.611206Z","iopub.execute_input":"2025-04-03T07:40:47.611591Z","iopub.status.idle":"2025-04-03T07:40:56.334335Z","shell.execute_reply.started":"2025-04-03T07:40:47.611553Z","shell.execute_reply":"2025-04-03T07:40:56.333249Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.7/144.7 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.9/100.9 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"'1.7.0'"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\n\nGOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\n\nclient = genai.Client(api_key=GOOGLE_API_KEY)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T07:40:56.335793Z","iopub.execute_input":"2025-04-03T07:40:56.336411Z","iopub.status.idle":"2025-04-03T07:40:56.899473Z","shell.execute_reply.started":"2025-04-03T07:40:56.336372Z","shell.execute_reply":"2025-04-03T07:40:56.898457Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"**Get the News Data:** We download a dataset of news articles, split into training and testing parts.\n\n**See the Categories:** We look at the list of news topics (like sports, science, etc.).\n\n**Show an Example:** We print out the text of the first news article.","metadata":{}},{"cell_type":"code","source":"from sklearn.datasets import fetch_20newsgroups\n\nnewsgroups_train = fetch_20newsgroups(subset=\"train\")\nnewsgroups_test = fetch_20newsgroups(subset=\"test\")\n\n# View list of class names for dataset\nnewsgroups_train.target_names\n\nprint(newsgroups_train.data[0])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T07:41:42.477629Z","iopub.execute_input":"2025-04-03T07:41:42.478024Z","iopub.status.idle":"2025-04-03T07:41:43.119357Z","shell.execute_reply.started":"2025-04-03T07:41:42.477993Z","shell.execute_reply":"2025-04-03T07:41:43.118333Z"}},"outputs":[{"name":"stdout","text":"From: lerxst@wam.umd.edu (where's my thing)\nSubject: WHAT car is this!?\nNntp-Posting-Host: rac3.wam.umd.edu\nOrganization: University of Maryland, College Park\nLines: 15\n\n I was wondering if anyone out there could enlighten me on this car I saw\nthe other day. It was a 2-door sports car, looked to be from the late 60s/\nearly 70s. It was called a Bricklin. The doors were really small. In addition,\nthe front bumper was separate from the rest of the body. This is \nall I know. If anyone can tellme a model name, engine specs, years\nof production, where this car is made, history, or whatever info you\nhave on this funky looking car, please e-mail.\n\nThanks,\n- IL\n   ---- brought to you by your neighborhood Lerxst ----\n\n\n\n\n\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"### Start by preprocessing the data for this tutorial in a Pandas dataframe. \nTo remove any sensitive information like names and email addresses, you will take only the subject and body of each message. This is an optional step that transforms the input data into more generic text, rather than email posts, so that it will work in other contexts.","metadata":{}},{"cell_type":"code","source":"import email\nimport re\nimport pandas as pd\n\ndef preprocess_text(data):\n    # Extract subject and body, remove email addresses, and truncate to 5,000 characters\n    msg = email.message_from_string(data)\n    text = f\"{msg['Subject']}\\n\\n{msg.get_payload()}\"\n    return re.sub(r\"[\\w\\.-]+@[\\w\\.-]+\", \"\", text)[:5000]\n\ndef preprocess_newsgroup_data(dataset):\n    # Create DataFrame and preprocess text\n    df = pd.DataFrame({\"Text\": dataset.data, \"Label\": dataset.target})\n    df[\"Text\"] = df[\"Text\"].apply(preprocess_text)\n    df[\"Class Name\"] = df[\"Label\"].map(lambda l: dataset.target_names[l])\n    return df\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T07:44:36.437477Z","iopub.execute_input":"2025-04-03T07:44:36.438244Z","iopub.status.idle":"2025-04-03T07:44:36.445158Z","shell.execute_reply.started":"2025-04-03T07:44:36.438204Z","shell.execute_reply":"2025-04-03T07:44:36.443973Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Apply preprocessing function to training and test datasets\ndf_train = preprocess_newsgroup_data(newsgroups_train)\ndf_test = preprocess_newsgroup_data(newsgroups_test)\n\ndf_train.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T07:44:55.165602Z","iopub.execute_input":"2025-04-03T07:44:55.166000Z","iopub.status.idle":"2025-04-03T07:44:58.903054Z","shell.execute_reply.started":"2025-04-03T07:44:55.165967Z","shell.execute_reply":"2025-04-03T07:44:58.902103Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"                                                Text  Label  \\\n0  WHAT car is this!?\\n\\n I was wondering if anyo...      7   \n1  SI Clock Poll - Final Call\\n\\nA fair number of...      4   \n2  PB questions...\\n\\nwell folks, my mac plus fin...      4   \n3  Re: Weitek P9000 ?\\n\\nRobert J.C. Kyanko () wr...      1   \n4  Re: Shuttle Launch Question\\n\\nFrom article <>...     14   \n\n              Class Name  \n0              rec.autos  \n1  comp.sys.mac.hardware  \n2  comp.sys.mac.hardware  \n3          comp.graphics  \n4              sci.space  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Text</th>\n      <th>Label</th>\n      <th>Class Name</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>WHAT car is this!?\\n\\n I was wondering if anyo...</td>\n      <td>7</td>\n      <td>rec.autos</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>SI Clock Poll - Final Call\\n\\nA fair number of...</td>\n      <td>4</td>\n      <td>comp.sys.mac.hardware</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>PB questions...\\n\\nwell folks, my mac plus fin...</td>\n      <td>4</td>\n      <td>comp.sys.mac.hardware</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Re: Weitek P9000 ?\\n\\nRobert J.C. Kyanko () wr...</td>\n      <td>1</td>\n      <td>comp.graphics</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Re: Shuttle Launch Question\\n\\nFrom article &lt;&gt;...</td>\n      <td>14</td>\n      <td>sci.space</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":8},{"cell_type":"markdown","source":"Next, you will sample some of the data by taking 100 data points in the training dataset, and dropping a few of the categories to run through this tutorial. Choose the science categories to compare.","metadata":{}},{"cell_type":"code","source":"def sample_data(df, num_samples, classes_to_keep):\n    # Sample rows, selecting num_samples of each Label.\n    df = (\n        df.groupby(\"Label\")[df.columns]\n        .apply(lambda x: x.sample(num_samples))\n        .reset_index(drop=True)\n    )\n\n    df = df[df[\"Class Name\"].str.contains(classes_to_keep)]\n\n    # We have fewer categories now, so re-calibrate the label encoding.\n    df[\"Class Name\"] = df[\"Class Name\"].astype(\"category\")\n    df[\"Encoded Label\"] = df[\"Class Name\"].cat.codes\n\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T07:45:47.084133Z","iopub.execute_input":"2025-04-03T07:45:47.084470Z","iopub.status.idle":"2025-04-03T07:45:47.091039Z","shell.execute_reply.started":"2025-04-03T07:45:47.084446Z","shell.execute_reply":"2025-04-03T07:45:47.089690Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"TRAIN_NUM_SAMPLES = 100\nTEST_NUM_SAMPLES = 25\n# Class name should contain 'sci' to keep science categories.\n# Try different labels from the data - see newsgroups_train.target_names\nCLASSES_TO_KEEP = \"sci\"\n\ndf_train = sample_data(df_train, TRAIN_NUM_SAMPLES, CLASSES_TO_KEEP)\ndf_test = sample_data(df_test, TEST_NUM_SAMPLES, CLASSES_TO_KEEP)\n\ndf_train.value_counts(\"Class Name\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T07:46:21.463651Z","iopub.execute_input":"2025-04-03T07:46:21.464077Z","iopub.status.idle":"2025-04-03T07:46:21.487964Z","shell.execute_reply.started":"2025-04-03T07:46:21.464047Z","shell.execute_reply":"2025-04-03T07:46:21.486841Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"Class Name\nsci.crypt          100\nsci.electronics    100\nsci.med            100\nsci.space          100\nName: count, dtype: int64"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"df_test.value_counts(\"Class Name\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T07:46:18.668446Z","iopub.execute_input":"2025-04-03T07:46:18.668872Z","iopub.status.idle":"2025-04-03T07:46:18.678367Z","shell.execute_reply.started":"2025-04-03T07:46:18.668840Z","shell.execute_reply":"2025-04-03T07:46:18.677273Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"Class Name\nsci.crypt          25\nsci.electronics    25\nsci.med            25\nsci.space          25\nName: count, dtype: int64"},"metadata":{}}],"execution_count":13},{"cell_type":"markdown","source":"## Create the embeddings\n\n**Turn Text into Numbers:** We'll use the Gemini API to convert each news article into a list of numbers, called \"embeddings.\" These numbers represent the meaning of the text.\n\n**Tell Gemini Our Goal:** We'll specify that we're using these embeddings for \"classification,\" meaning we want to sort the articles into categories. This helps Gemini generate the most useful numbers.\n\n**One by One:** The API processes each article separately, creating a number list for each one. This might take a while if you have many articles.","metadata":{}},{"cell_type":"code","source":"from google.api_core import retry\nimport tqdm\nfrom tqdm.rich import tqdm as tqdmr\nimport warnings\n\n# Enable progress bars for Pandas and suppress warnings\ntqdmr.pandas()\n\nwarnings.filterwarnings(\"ignore\", category=tqdm.TqdmExperimentalWarning)\n\n@retry.Retry(\n    predicate=lambda e: isinstance(e, genai.errors.APIError) and e.code in {429, 503},\n    timeout=300\n)\ndef embed_text(text: str) -> list[float]:\n    \"\"\"Generate embeddings for a given text using the specified model.\"\"\"\n    response = client.models.embed_content(\n        model=\"models/text-embedding-004\",\n        contents=text,\n        config=types.EmbedContentConfig(task_type=\"classification\")\n    )\n    return response.embeddings[0].values\n\ndef create_embeddings(df):\n    \"\"\"Add embeddings to the DataFrame based on the 'Text' column.\"\"\"\n    df[\"Embeddings\"] = df[\"Text\"].progress_apply(embed_text)\n    return df\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T08:00:04.529848Z","iopub.execute_input":"2025-04-03T08:00:04.530265Z","iopub.status.idle":"2025-04-03T08:00:04.537895Z","shell.execute_reply.started":"2025-04-03T08:00:04.530233Z","shell.execute_reply":"2025-04-03T08:00:04.536650Z"}},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":"**Explanation:**\nWe're taking our training and testing data, and we're adding columns to them that contain the numerical representations of the article text, so that our machine learning model can understand them.","metadata":{}},{"cell_type":"code","source":"df_train = create_embeddings(df_train)\ndf_test = create_embeddings(df_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T08:00:19.371599Z","iopub.execute_input":"2025-04-03T08:00:19.372052Z","iopub.status.idle":"2025-04-03T08:06:47.767487Z","shell.execute_reply.started":"2025-04-03T08:00:19.372020Z","shell.execute_reply":"2025-04-03T08:06:47.766251Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output()","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"562bade54ac74461b982b09a9ebb90a1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Output()","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"64f34dda1e414b7aadd69cd2b5b36f8a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"df_train.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T08:06:47.772648Z","iopub.execute_input":"2025-04-03T08:06:47.773061Z","iopub.status.idle":"2025-04-03T08:06:47.791114Z","shell.execute_reply.started":"2025-04-03T08:06:47.773022Z","shell.execute_reply":"2025-04-03T08:06:47.789772Z"}},"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"                                                Text  Label Class Name  \\\n0  Re: Clipper considered harmful\\n\\nIn article <...     11  sci.crypt   \n1  Re: Once tapped, your code is no good any more...     11  sci.crypt   \n2  Cripple Chip\\n\\nHow about this:  The\\nTelCo ha...     11  sci.crypt   \n3  Re: clipper chip --Bush did it\\n\\n (John Gilbe...     11  sci.crypt   \n4  Clipper chip -- technical details\\n\\nI receive...     11  sci.crypt   \n\n   Encoded Label                                         Embeddings  \n0              0  [0.004346496, 0.029194878, -0.06622962, 0.0297...  \n1              0  [-0.017911352, 0.017148165, -0.04155469, -0.00...  \n2              0  [-0.013676126, 0.03906149, -0.041958194, 0.003...  \n3              0  [-0.0047690864, 0.024499344, -0.039486606, 0.0...  \n4              0  [0.01475392, 0.020964768, -0.057706054, 0.0120...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Text</th>\n      <th>Label</th>\n      <th>Class Name</th>\n      <th>Encoded Label</th>\n      <th>Embeddings</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Re: Clipper considered harmful\\n\\nIn article &lt;...</td>\n      <td>11</td>\n      <td>sci.crypt</td>\n      <td>0</td>\n      <td>[0.004346496, 0.029194878, -0.06622962, 0.0297...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Re: Once tapped, your code is no good any more...</td>\n      <td>11</td>\n      <td>sci.crypt</td>\n      <td>0</td>\n      <td>[-0.017911352, 0.017148165, -0.04155469, -0.00...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Cripple Chip\\n\\nHow about this:  The\\nTelCo ha...</td>\n      <td>11</td>\n      <td>sci.crypt</td>\n      <td>0</td>\n      <td>[-0.013676126, 0.03906149, -0.041958194, 0.003...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Re: clipper chip --Bush did it\\n\\n (John Gilbe...</td>\n      <td>11</td>\n      <td>sci.crypt</td>\n      <td>0</td>\n      <td>[-0.0047690864, 0.024499344, -0.039486606, 0.0...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Clipper chip -- technical details\\n\\nI receive...</td>\n      <td>11</td>\n      <td>sci.crypt</td>\n      <td>0</td>\n      <td>[0.01475392, 0.020964768, -0.057706054, 0.0120...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":25},{"cell_type":"markdown","source":"\n## Build a classification model¶\n\nHere you will define a simple model that accepts the **raw embedding data** as input, has one **hidden layer**, and an **output layer** specifying the class probabilities. The prediction will correspond to the probability of a piece of text being a particular class of news.\n\nWhen you run the model, Keras will take care of details like **shuffling the data points** , calculating metrics and other ML boilerplate.\n\nWe'll build a simple **\"sorting machine\" (model)** that takes the article's **number codes (embeddings) as input**. It **learns patterns** in these codes to **guess the article's category**, giving us a **probability for each category**. **Keras** handles the **learning process**.","metadata":{}},{"cell_type":"code","source":"import keras\nfrom keras import Sequential, layers\n\ndef build_classification_model(input_dim: int, num_classes: int) -> Sequential:\n    return Sequential([\n        layers.Dense(input_dim, activation='relu', input_shape=(input_dim,)),\n        layers.Dense(num_classes, activation='softmax')\n    ])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T08:15:13.215523Z","iopub.execute_input":"2025-04-03T08:15:13.215963Z","iopub.status.idle":"2025-04-03T08:15:13.221457Z","shell.execute_reply.started":"2025-04-03T08:15:13.215925Z","shell.execute_reply":"2025-04-03T08:15:13.220236Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"# Derive the embedding size from observing the data. The embedding size can also be specified\n# with the `output_dimensionality` parameter to `embed_content` if you need to reduce it.\nembedding_size = len(df_train[\"Embeddings\"].iloc[0])\n\nclassifier = build_classification_model(\n    embedding_size, len(df_train[\"Class Name\"].unique())\n)\nclassifier.summary()\n\nclassifier.compile(\n    loss=keras.losses.SparseCategoricalCrossentropy(),\n    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n    metrics=[\"accuracy\"],\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T08:15:15.384320Z","iopub.execute_input":"2025-04-03T08:15:15.384719Z","iopub.status.idle":"2025-04-03T08:15:15.439308Z","shell.execute_reply.started":"2025-04-03T08:15:15.384685Z","shell.execute_reply":"2025-04-03T08:15:15.438083Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential_1\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)                 │         \u001b[38;5;34m590,592\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)                   │           \u001b[38;5;34m3,076\u001b[0m │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">590,592</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)                   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">3,076</span> │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m593,668\u001b[0m (2.26 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">593,668</span> (2.26 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m593,668\u001b[0m (2.26 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">593,668</span> (2.26 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}}],"execution_count":34},{"cell_type":"markdown","source":"## Train the model\nFinally, you can train your model. This code uses early stopping to exit the training loop once the loss value stabilises, so the number of epoch loops executed may differ from the specified value.","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\n\nNUM_EPOCHS = 20\nBATCH_SIZE = 32\n\n# Split the x and y components of the train and validation subsets.\ny_train = df_train[\"Encoded Label\"]\nx_train = np.stack(df_train[\"Embeddings\"])\ny_val = df_test[\"Encoded Label\"]\nx_val = np.stack(df_test[\"Embeddings\"])\n\n# Specify that it's OK to stop early if accuracy stabilises.\nearly_stop = keras.callbacks.EarlyStopping(monitor=\"accuracy\", patience=3)\n\n# Train the model for the desired number of epochs.\nhistory = classifier.fit(\n    x=x_train,\n    y=y_train,\n    validation_data=(x_val, y_val),\n    callbacks=[early_stop],\n    batch_size=BATCH_SIZE,\n    epochs=NUM_EPOCHS,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T08:15:18.877471Z","iopub.execute_input":"2025-04-03T08:15:18.877859Z","iopub.status.idle":"2025-04-03T08:15:22.338680Z","shell.execute_reply.started":"2025-04-03T08:15:18.877830Z","shell.execute_reply":"2025-04-03T08:15:22.337053Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/20\n\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.2805 - loss: 1.3620 - val_accuracy: 0.7000 - val_loss: 1.2559\nEpoch 2/20\n\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5739 - loss: 1.2086 - val_accuracy: 0.7000 - val_loss: 1.1130\nEpoch 3/20\n\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8556 - loss: 1.0317 - val_accuracy: 0.9200 - val_loss: 0.9373\nEpoch 4/20\n\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9657 - loss: 0.7927 - val_accuracy: 0.9000 - val_loss: 0.7662\nEpoch 5/20\n\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9698 - loss: 0.6332 - val_accuracy: 0.9100 - val_loss: 0.6259\nEpoch 6/20\n\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9784 - loss: 0.4799 - val_accuracy: 0.9200 - val_loss: 0.5250\nEpoch 7/20\n\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9726 - loss: 0.3715 - val_accuracy: 0.9400 - val_loss: 0.4345\nEpoch 8/20\n\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9853 - loss: 0.2727 - val_accuracy: 0.9500 - val_loss: 0.3890\nEpoch 9/20\n\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9844 - loss: 0.2244 - val_accuracy: 0.9400 - val_loss: 0.3400\nEpoch 10/20\n\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9912 - loss: 0.1764 - val_accuracy: 0.9400 - val_loss: 0.3194\nEpoch 11/20\n\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9895 - loss: 0.1464 - val_accuracy: 0.9500 - val_loss: 0.3159\nEpoch 12/20\n\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9978 - loss: 0.1247 - val_accuracy: 0.9100 - val_loss: 0.2916\nEpoch 13/20\n\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9949 - loss: 0.1098 - val_accuracy: 0.9500 - val_loss: 0.2796\nEpoch 14/20\n\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9995 - loss: 0.0865 - val_accuracy: 0.9400 - val_loss: 0.2651\nEpoch 15/20\n\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9956 - loss: 0.0868 - val_accuracy: 0.9500 - val_loss: 0.2744\n","output_type":"stream"}],"execution_count":35},{"cell_type":"markdown","source":"## Evaluate model performance\nUse Keras Model.evaluate to calculate the loss and accuracy on the test dataset.","metadata":{}},{"cell_type":"code","source":"classifier.evaluate(x=x_val, y=y_val, return_dict=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T08:15:25.847395Z","iopub.execute_input":"2025-04-03T08:15:25.847758Z","iopub.status.idle":"2025-04-03T08:15:25.946381Z","shell.execute_reply.started":"2025-04-03T08:15:25.847712Z","shell.execute_reply":"2025-04-03T08:15:25.945415Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9623 - loss: 0.2181 \n","output_type":"stream"},{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"{'accuracy': 0.949999988079071, 'loss': 0.2744240164756775}"},"metadata":{}}],"execution_count":36},{"cell_type":"markdown","source":"## Try a custom prediction¶\nNow that you have a trained model with good evaluation metrics, you can try to make a prediction with new, hand-written data. Use the provided example or try your own data to see how the model performs.","metadata":{}},{"cell_type":"code","source":"def make_prediction(text: str) -> list[float]:\n    \"\"\"Infer categories from the provided text.\"\"\"\n    # Remember that the model takes embeddings as input, so calculate them first.\n    embedded = embed_fn(new_text)\n\n    # And recall that the input must be batched, so here they are wrapped as a\n    # list to provide a batch of 1.\n    inp = np.array([embedded])\n\n    # And un-batched here.\n    [result] = classifier.predict(inp)\n    return result","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T08:15:28.968276Z","iopub.execute_input":"2025-04-03T08:15:28.968634Z","iopub.status.idle":"2025-04-03T08:15:28.974092Z","shell.execute_reply.started":"2025-04-03T08:15:28.968607Z","shell.execute_reply":"2025-04-03T08:15:28.972888Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"# This example avoids any space-specific terminology to see if the model avoids\n# biases towards specific jargon.\nnew_text = \"\"\"\nFirst-timer looking to get out of here.\n\nHi, I'm writing about my interest in travelling to the outer limits!\n\nWhat kind of craft can I buy? What is easiest to access from this 3rd rock?\n\nLet me know how to do that please.\n\"\"\"\n\nresult = make_prediction(new_text)\n\nfor idx, category in enumerate(df_test[\"Class Name\"].cat.categories):\n    print(f\"{category}: {result[idx] * 100:0.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T08:15:31.137552Z","iopub.execute_input":"2025-04-03T08:15:31.137977Z","iopub.status.idle":"2025-04-03T08:15:31.518905Z","shell.execute_reply.started":"2025-04-03T08:15:31.137944Z","shell.execute_reply":"2025-04-03T08:15:31.517570Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\nsci.crypt: 0.01%\nsci.electronics: 1.28%\nsci.med: 0.09%\nsci.space: 98.61%\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}